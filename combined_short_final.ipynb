{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the complixty_diff function so we can run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from autocorrect import spell #import spell checker\n",
    "import numpy as np\n",
    "from textstat.textstat import textstat #import vocabulary level grader\n",
    "from sklearn.cross_validation import train_test_split #for training and testing split\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import csv\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "#this block of code is for preprocessing the data you only need to run this once\n",
    "def clean_Essay( raw_review ):\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [spell(stemmer.stem(w)) for w in words if not w in stops]   \n",
    "    # 6. Doing a spell corrector\n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organization(ids,essays,final_features):\n",
    "    ##df = pd.read_csv(file_path)\n",
    "    ##ids = df[\"essay_id\"]\n",
    "  \n",
    "    #PRE-PROCESSING\n",
    "    essays = df[\"EssayText\"].str.lower()\n",
    "    #removing puctuation \n",
    "    for i,m in zip(ids,essays):\n",
    "        essays[i] = \" \".join(c for c in word_tokenize(m) if c not in list(string.punctuation))\n",
    "  \n",
    "    #organization words (n\n",
    "    unigram_org= {\"begin\":1, \"first\":1, \"firstly\":1, \"initially\":1, \"foremost\":1,\"conclusion\":1, \"conclude\":1, \"final\":1, \"finally\":1, \"last\":1, \"lastly\":1, \"ultimately\":1, \"end\":1, \"sum\":1, \"eventually\":1, \"so\":1, \"thus\":1, \"hence\":1, \"altogether\":1, \"summarize\":1, \"summary\":1, \"therefore\":1, \"overall\":1, \"secondly\":1, \"next\":1, \"subsequently\":1, \"before\":1, \"previously\": 1, \"afterwards\":1, \"then\":1, \"after\":1, \"so\":1, \"example\":1, \"instance\":1, \"because\":1, \"consequently\":1, \"consequence\":1, \"therefore\":1, \"result\":1, \"due\":1, \"rather\":1, \"however\":1, \"moreover\":1, \"nonetheless\":1, \"still\":1, \"yet\":1, \"nevertheless\":1, \"although\":1, \"though\":1, \"regardless\":1, \"despite\":1, \"indeed\":1, \"importantly\":1, \"besides\":1, \"contrast\":1, \"while\":1, \"conversely\":1, \"similarly\":1, \"likewise\":1, \"equally\":1, \"namely\":1, \"specifically\":1, \"especially\":1, \"particularly\":1, \"illustrated\":1, \"illustrates\":1, \"also\":1, \"and\":1, \"or\":1, \"too\":1, \"addition\":1, \"furthermore\":1, \"further\":1, \"alternatively\":1}\n",
    "    bigram_org ={\"i think\":1, \"in brief\":1,\"in conclusion\":1,\"to conclude\":1,\"to summarize\":1,\"in sum\":1,\"in summary\":1,\"Above all\":1,\"Coupled with\":1, \"Whats more\":1}\n",
    "    trigram_org={\"in order to\":1,\"in other words\":1, \"to that end\":1, \"as well as\":1, \"not to mention\":1, \"in the end\":1, \"on the whole\":1, \"to sum up\":1, \"an additional info\":1}\n",
    "\n",
    "    scores={}\n",
    "    #calculating number of unigram org words\n",
    "    for k,j in zip(ids,essays):\n",
    "        points = 0\n",
    "        if(type(j)!=float):\n",
    "            for i in j.split():\n",
    "                if i in unigram_org:\n",
    "                    points = points + unigram_org[i]\n",
    "        scores[k] = points\n",
    "\n",
    "    #creating the bigrams and trigrams \n",
    "    for k,j in zip(ids,essays):\n",
    "        bigrams = []\n",
    "        trigrams = []\n",
    "        n = nltk.word_tokenize(j)\n",
    "        bi = ngrams(n,2)\n",
    "        tri = ngrams(n,3)\n",
    "        bigrams=[' '.join(i) for i in list(bi)]\n",
    "        trigrams=[' '.join(i) for i in list(tri)]\n",
    "\n",
    "    #calculating number of bigram org words\n",
    "        points= 0\n",
    "        if(type(j)!=float):\n",
    "            for x in bigrams:\n",
    "                if x in bigram_org:\n",
    "                    points = points + bigram_org[x]\n",
    "        scores[k] = scores[k] + points\n",
    "    \n",
    "    #calculating number of trigram org words\n",
    "        points= 0\n",
    "        essay_len=len(j)\n",
    "        if(type(j)!=float):\n",
    "            for z in trigrams:\n",
    "                if z in trigram_org:\n",
    "                    points = points + trigram_org[z]\n",
    "        scores[k] = scores[k] + points\n",
    "        scores[k] = scores[k]/essay_len\n",
    "        \n",
    "    dataframe = pd.DataFrame(list(scores.items()))\n",
    "    dataframe[2] = (dataframe[1] - dataframe[1].mean()) / (dataframe[1].max() - dataframe[1].min())\n",
    "    bins = np.linspace(dataframe[2].min(), dataframe[2].max(), 4)\n",
    "    \n",
    "    low=[]\n",
    "    mid=[]\n",
    "    high=[]\n",
    "    def grader(x):\n",
    "    \n",
    "        if x <= bins[1]:\n",
    "            low.append(x)\n",
    "            return 0\n",
    "        \n",
    "        elif x > bins[1] or x < bins[2]:\n",
    "            mid.append(x)\n",
    "            return 1\n",
    "        \n",
    "        elif x > bins[2]:\n",
    "            high.append(x)\n",
    "            return 2\n",
    "        \n",
    "    dataframe['GRADE'] =None\n",
    "    dataframe['GRADE'] = dataframe[2].map(grader)\n",
    "    \n",
    "    '''\n",
    "    for k in dataframe[0]:\n",
    "        final_features[k]['organization'] = dataframe['GRADE'][k]\n",
    "    '''\n",
    "    \n",
    "    new_ids = dataframe[0]\n",
    "    grades = dataframe[\"GRADE\"]\n",
    "    for i,j in zip(new_ids,grades):\n",
    "        final_features[i][\"organization\"] = j\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_POS(essay_text):\n",
    "    \"\"\"Tags each word in the essay with its part of speech. The ratios of verbs, nouns, and adjectives may be a useful\n",
    "    feature - for instance, questions that ask students to 'describe' things may use more adjectives, while questions\n",
    "    that ask students to 'list' things may use more nouns. We can use either the direct counts or the ratios as features.\n",
    "    The method returns the ratios (# NN|VB|JJ / total words).\n",
    "    Note: Do this before removing stop words.\"\"\"\n",
    "    #print(essay_text)\n",
    "    \n",
    "    # split essay text into words\n",
    "    wds = nltk.tokenize.word_tokenize(essay_text)\n",
    "    #total_wds = len(wds) # this is not the true length, as it may include some tagged puntuation\n",
    "    #print(wds)\n",
    "    #print(\"Total words: \", total_wds)\n",
    "    \n",
    "    # tag POS\n",
    "    tagged = nltk.pos_tag(wds)\n",
    "    #print(tagged)\n",
    "    \n",
    "    # counters\n",
    "    adj_advb = 0 # JJ, JJR, JJS are adjectives/descriptors; RB, RBR, RBS are adverbs (also descriptors)\n",
    "    nn = 0 # NN,NNS, NNP, NNPS are nouns (both proper and common, singular and plural)\n",
    "    pn = 0 # just proper nouns\n",
    "    vb = 0 # VB, VBD, VBG, VBN, VBP, VBZ are verbs in various tenses\n",
    "    nums = 0 # CD: numerical values found in the text\n",
    "    other = 0\n",
    "    total_wds = 0\n",
    "    \n",
    "    # count POS and total words\n",
    "    for w in tagged:\n",
    "        pos = w[1]\n",
    "        #print(\"Part of speech for \", w[0], \" is \", pos)\n",
    "        if (pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS'):\n",
    "            # adjective or adverb\n",
    "            adj_advb += 1\n",
    "            total_wds += 1\n",
    "        elif (pos == 'NN' or pos == 'NNS' or pos == 'NNP' or pos == 'NNPS'):\n",
    "            # common and proper nouns\n",
    "            nn += 1\n",
    "            total_wds += 1\n",
    "            if (pos == 'NNP' or pos == 'NNPS'):\n",
    "                # proper nouns only\n",
    "                pn += 1\n",
    "        elif(pos == 'VB' or pos == 'VBD' or pos == 'VBG' or pos == 'VBN' or pos == 'VBP' or pos == 'VBZ'):\n",
    "            # all verb forms\n",
    "            vb += 1\n",
    "            total_wds += 1\n",
    "        elif(pos == 'CD'):\n",
    "            # numerical values like years and measurements, we can count these as words\n",
    "            nums += 1\n",
    "            total_wds += 1\n",
    "        elif(pos == '$' or pos == '.' or pos == '(' or pos == ')' or pos == \"''\" or pos == ',' or pos == '--'\n",
    "            or pos == ',' or pos == ':' or pos == 'SYM' or pos == \"``\"):\n",
    "            # symbols and punctuation, not counted as words\n",
    "            pass # don't count anything\n",
    "        else:\n",
    "            # all other words\n",
    "            other += 1\n",
    "            total_wds += 1\n",
    "            \n",
    "    # ratios of descriptors, nouns, proper nouns, verbs\n",
    "    adjadvb_ratio = adj_advb/total_wds\n",
    "    n_ratio = nn/total_wds\n",
    "    pn_ratio = pn/total_wds\n",
    "    vb_ratio = vb/total_wds\n",
    "    other_ratio = other/total_wds\n",
    "    # also return number of numerical values found (may be more useful than ratio, particularly for questions that require some \n",
    "    #    numerical response)\n",
    "    \n",
    "    return [adjadvb_ratio, n_ratio, pn_ratio, vb_ratio, other_ratio, nums]\n",
    "\n",
    "def POS_diff(setnum, pos_vals, base_df):\n",
    "    \"\"\"Calculates and returns the difference between the POS value and the baseline for good essays\"\"\"\n",
    "    diffs = []\n",
    "    diffs.append(abs(pos_vals[0] - base_df.loc[setnum-1]['Adjadv']))\n",
    "    diffs.append(abs(pos_vals[1] - base_df.loc[setnum-1]['Noun']))\n",
    "    diffs.append(abs(pos_vals[2] - base_df.loc[setnum-1]['Pronoun']))\n",
    "    diffs.append(abs(pos_vals[3] - base_df.loc[setnum-1]['Verb']))\n",
    "    diffs.append(abs(pos_vals[4] - base_df.loc[setnum-1]['Other']))\n",
    "    diffs.append(abs(pos_vals[5] - base_df.loc[setnum-1]['Nums']))\n",
    "    return diffs\n",
    "def Complexity_diff(setnum, comp_vals, base_df):\n",
    "    \"\"\"Calculates and returns the difference between the complexity values and the baseline for good essays\"\"\"\n",
    "    diffs = []\n",
    "    diffs.append(abs(comp_vals[0] - base_df.loc[setnum-1]['Short']))\n",
    "    diffs.append(abs(comp_vals[1] - base_df.loc[setnum-1]['Medium']))\n",
    "    diffs.append(abs(comp_vals[2] - base_df.loc[setnum-1]['Long']))\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complexity(essay_text):\n",
    "    \"\"\"Calculates the length of each sentence in the essay (may only be applicable to longer essay responses, \n",
    "    not short response answers). \n",
    "    Counts the number of short (<=10 words), medium (11 - 35 words), and long (>35 words) sentences.\n",
    "    The ratios of these is used as a measure of style (good writing has a mix of sentence lengths), and may \n",
    "    be correlated with score of the response. These ratios (# size / total sentences) are returned as a list.\n",
    "    Note: to get an accurate sentence count, this function should be called before removing stop words.\"\"\"\n",
    "    # BEFORE removing stop words\n",
    "    #print(essay_text)\n",
    "    \n",
    "    # counters for sentence lengths\n",
    "    short = 0\n",
    "    med = 0\n",
    "    long = 0\n",
    "    \n",
    "    # split essay text into sentences\n",
    "    sents = nltk.tokenize.sent_tokenize(essay_text)\n",
    "    #print(sents)\n",
    "    \n",
    "    # for each sentence, count number of words and increment appropriate length counter\n",
    "    for s in sents:\n",
    "        #print(\"Sentence: \", s)\n",
    "        wds = nltk.tokenize.word_tokenize(s)\n",
    "        #print(\"Word list: \", wds)\n",
    "        l = 0 # store sentence length\n",
    "        for i in wds:\n",
    "            if i not in list(string.punctuation):\n",
    "                l += 1\n",
    "                \n",
    "        #print(\"Length=\", l)\n",
    "        if l <= 10:\n",
    "            short += 1\n",
    "        elif l <= 35:\n",
    "            med += 1\n",
    "        else:\n",
    "            long += 1\n",
    "    \n",
    "    # results that can be used as features\n",
    "    #print(\"This essay has:\")\n",
    "    #print(short, \" short sentences\")\n",
    "    #print(med, \" medium sentences\")\n",
    "    #print(long, \" long sentences\")\n",
    "    \n",
    "    # better essays tend to have a balanced mixture of sentence lengths\n",
    "    # an equal number of each category (1/3 each) to 1/2 medium, 1/4 short, 1/4 long\n",
    "    # we could either use one feature for this (a number or ratio indicating amount of balance)\n",
    "    # or we could have three features, one ratio for each length\n",
    "    total = short + med + long\n",
    "    # return three ratios for three features\n",
    "    return [short/total, med/total, long/total]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"All_test_Norm_clean.csv\", index_col=False)\n",
    "df.dropna(axis=0,how='any', inplace=True)\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer',  CountVectorizer()),\n",
    "        ('tfidf_transformer',  TfidfTransformer()),\n",
    "        ('classifier',  RandomForestClassifier(n_estimators=100))])\n",
    "pipeline.fit(df['EssayText'].values, df['Norm_score1'].values)\n",
    "def pred_score(text):\n",
    "    text = clean_Essay(text)\n",
    "    essay= [text]\n",
    "#if you just want to load the dataframe and see results then call this\n",
    "#just make sure that English_clean.csv is in the same directory you are in\n",
    "    global df\n",
    "    global pipeline\n",
    "    return pipeline.predict(essay)[0]# produce predicted score\n",
    "\n",
    "def vocab_grade(text):\n",
    "    return textstat.automated_readability_index(text)\n",
    "\n",
    "def prep_prompt(prompt_path):\n",
    "    df = pd.read_csv(prompt_path)\n",
    "    prompts = df[\"prompt\"].str.lower()\n",
    "    for m in range(len(prompts)):\n",
    "        prompts[m] = \" \".join(c for c in word_tokenize(prompts[m]) if c not in list(string.punctuation))\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for m in range(len(prompts)):\n",
    "        prompts[m] = \" \".join(c for c in word_tokenize(prompts[m]) if c not in list(stop))\n",
    "    return prompts\n",
    "\n",
    "def essay_length(ids,essays,final_features):\n",
    "    for m in range(len(essays)):\n",
    "        essays[m] = \" \".join(c for c in word_tokenize(essays[m]) if c not in list(string.punctuation))\n",
    "    \n",
    "    for k,j in zip(ids,essays):\n",
    "        length = 0\n",
    "        for x in j.split():\n",
    "            length += 1 \n",
    "        final_features[k][\"total_length\"]=length\n",
    "        \n",
    "def prompt_relevance(ids,essays,sets,final_features):\n",
    "    for m in range(len(essays)):\n",
    "        essays[m] = \" \".join(c for c in word_tokenize(essays[m]) if c not in list(string.punctuation))\n",
    "  \n",
    "    prompts = prep_prompt(\"short_prompts.csv\")\n",
    "    for k,j,i in zip(ids,essays,sets):\n",
    "        set_index = i-1\n",
    "        points = 0\n",
    "        if(type(j)!=float):\n",
    "            for i in j.split():\n",
    "                if i in prompts[set_index]:\n",
    "                    points = points + 1\n",
    "        final_features[k][\"prompt_relevance\"] = points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combined(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    ids = df[\"Id\"]\n",
    "    #PRE-PROCESSING\n",
    "    essays = df[\"EssayText\"].str.lower()\n",
    "    scores = df[\"Score1\"]\n",
    "    sets = df[\"EssaySet\"]\n",
    "    \n",
    "    final_features = {}\n",
    "    \n",
    "    print(\"adding given scores\")\n",
    "    for k,s in zip(ids,scores):\n",
    "        final_features[k] = {}\n",
    "        final_features[k][\"score\"] = s\n",
    "    \n",
    "    for k,s in zip(ids,sets):\n",
    "        final_features[k][\"set\"] = s\n",
    "        \n",
    "    print(\"running org\")\n",
    "    org_results = organization(ids,essays,final_features)\n",
    "    \n",
    "    print(\"running pos\")\n",
    "    # read baseline values for comparison\n",
    "    base = pd.read_csv(\"Short-POS-baselines.csv\")\n",
    "    for k,j in zip(ids,essays):\n",
    "        pos_score = tag_POS(j) # get original POS ratios\n",
    "        setnum = final_features[k][\"set\"]\n",
    "        diff = POS_diff(setnum, pos_score, base) #get (absolute value) difference from 'good' baseline\n",
    "        #put diffs into 3 buckets - close to baseline, farther from baseline, and farthest\n",
    "        # the distinction between buckets is arbitrary right now and could be refined\n",
    "        \n",
    "        i = 0\n",
    "        while(i < len(diff)):\n",
    "            if diff[i] < 0.005:\n",
    "                diff[i] = 3 #close\n",
    "            elif diff[i] < 1:\n",
    "                diff[i] = 2\n",
    "            else:\n",
    "                diff[i] = 1 #farthest\n",
    "            i += 1\n",
    "        \n",
    "        final_features[k][\"pos_adjadv\"] = diff[0]\n",
    "        final_features[k][\"pos_noun\"] = diff[1]\n",
    "        final_features[k][\"pos_pronoun\"] = diff[2]\n",
    "        final_features[k][\"pos_verb\"] = diff[3]\n",
    "        final_features[k][\"pos_other\"] = diff[4]\n",
    "        final_features[k][\"pos_nums\"] = diff[5]\n",
    "    \n",
    "    print(\"running complexity\")\n",
    "    compbase = pd.read_csv(\"Short-Complexity-baselines.csv\")\n",
    "    for k,j in zip(ids,essays):\n",
    "        complex_score = complexity(j) #get original complexity scores\n",
    "        setnum = final_features[k][\"set\"]\n",
    "        diff = Complexity_diff(setnum, complex_score, compbase)\n",
    "        #put diffs into 4 buckets - close to baseline, medium-close, medium-far, and farthest from baseline\n",
    "        # the distinction between buckets is arbitrary right now and could be refined\n",
    "        \n",
    "        i = 0\n",
    "        while(i < len(diff)):\n",
    "            if diff[i] < 0.05:\n",
    "                diff[i] = 4 #close\n",
    "            elif diff[i] < 0.10:\n",
    "                diff[i] = 3\n",
    "            elif diff[i] < 0.50:\n",
    "                diff[i] = 2\n",
    "            else:\n",
    "                diff[i] = 1 #farthest\n",
    "            i += 1\n",
    "            \n",
    "        final_features[k][\"complex_short\"] = diff[0]\n",
    "        final_features[k][\"complex_medium\"] = diff[1]\n",
    "        final_features[k][\"complex_long\"] = diff[2]\n",
    "    i=0\n",
    "    \n",
    "    print(\"running pred_ score and vocab\")\n",
    "    for k,j in zip(ids,essays):\n",
    "        if(i % 100 == 0):    \n",
    "            print(i)\n",
    "        pscore = pred_score(j)\n",
    "        final_features[k][\"pred_score\"] = pscore\n",
    "        \n",
    "        vocab = vocab_grade(j)\n",
    "        final_features[k][\"vocab_level\"] = vocab\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "    print(\"running prompt relevance\")\n",
    "    \n",
    "    prompt_relevance(ids,essays,sets,final_features)\n",
    "    \n",
    "    print(\"running total length feature\")\n",
    "    \n",
    "    essay_length(ids,essays,final_features)\n",
    "    print(\"FINISHED\")\n",
    "    \n",
    "    return final_features\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_to_csv(feature_dict):\n",
    "    for k in feature_dict:\n",
    "        filename = 'short_set_expr' + str(k) + '.csv'\n",
    "        with open(filename,'w') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow([\"essay_id\", \"set\", \"score\", \"org_score\", \"pos_adjadv\", \"pos_noun\", \"pos_pronoun\", \"pos_verb\", \"pos_other\", \"pos_nums\", \"complex_short\", \"complex_medium\", \"complex_long\", \"vocab_level\",\"pred_score\",\"prompt_relevence\",\"total_length\"])\n",
    "            for key in feature_dict[k]:\n",
    "                row = []\n",
    "                row.append(key)\n",
    "                row.append(k)\n",
    "                row.append(feature_dict[k][key][\"score\"])\n",
    "                row.append(feature_dict[k][key][\"organization\"])\n",
    "                row.append(feature_dict[k][key][\"pos_adjadv\"])\n",
    "                row.append(feature_dict[k][key][\"pos_noun\"])\n",
    "                row.append(feature_dict[k][key][\"pos_pronoun\"])\n",
    "                row.append(feature_dict[k][key][\"pos_verb\"])\n",
    "                row.append(feature_dict[k][key][\"pos_other\"])\n",
    "                row.append(feature_dict[k][key][\"pos_nums\"])\n",
    "                row.append(feature_dict[k][key][\"complex_short\"])\n",
    "                row.append(feature_dict[k][key][\"complex_medium\"])\n",
    "                row.append(feature_dict[k][key][\"complex_long\"])\n",
    "                row.append(feature_dict[k][key][\"vocab_level\"])\n",
    "                row.append(feature_dict[k][key][\"pred_score\"])\n",
    "                row.append(feature_dict[k][key][\"prompt_relevance\"])\n",
    "                row.append(feature_dict[k][key][\"total_length\"])\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    csvfile.close()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#split dataset by question set\n",
    "def split_by_set(features):\n",
    "    \"\"\"Returns a dictionary of sets mapped to a dictionary of essays in the set mapped to a dictionary of their features\"\"\"\n",
    "    set = {}\n",
    "    for k in features:\n",
    "        #print(features[k])\n",
    "        qset = features[k]['set']\n",
    "        #print(k, \" is in set \", qset)\n",
    "        d = {k : features[k]}\n",
    "        if qset in set:\n",
    "            set[qset].update(d)\n",
    "        else:\n",
    "            set.update({qset : d})\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding given scores\n",
      "running org\n",
      "running pos\n",
      "running complexity\n",
      "running pred_ score and vocab\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "running prompt relevance\n",
      "running total length feature\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "test=combined(\"All_Norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = split_by_set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_to_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"train_all_sets.csv\", index_col= False)\n",
    "df1[\"Norm_score1\"]= 0\n",
    "df1.loc[df1['Score1']>=2, \"Norm_score1\"]  = 1 \n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "df1.to_csv(\"All_Norm.csv\",index=False)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "df1['EssayText'] = df1['EssayText'].apply(lambda x: clean_Essay(x))#check cell above\n",
    "df1.dropna(axis=0,how='any', inplace=True)\n",
    "df1.to_csv(\"All_test_Norm_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
